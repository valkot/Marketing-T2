# Problem Set 02: Marketing y Anal√≠tica del Retail
**Mag√≠ster in Business Analytics and Data Science - UDP**

Este repositorio contiene la resoluci√≥n integral del Problem Set 02, enfocado en el procesamiento de datos no estructurados y el desarrollo de sistemas de recomendaci√≥n. El proyecto simula un entorno real de retail enfrentando desaf√≠os como ruido en los datos, textos sin procesar y matrices de consumo dispersas (sparsity).

## üìã √çndice
1. [Parte 1: NLP y Embeddings](#parte-1-nlp-y-embeddings)
2. [Parte 2: Modelado de T√≥picos (BERTopic)](#parte-2-modelado-de-t√≥picos)
3. [Parte 4: Recomendaci√≥n (Feedback Expl√≠cito)](#parte-3-recomendaci√≥n-expl√≠cita)
4. [Parte 4: Recomendaci√≥n (Feedback Impl√≠cito)](#parte-4-recomendaci√≥n-impl√≠cita)
5. [Requisitos T√©cnicos y Compatibilidad](#requisitos-t√©cnicos-y-compatibilidad)

---

## üß† Parte 1: NLP y Embeddings
Utilizando el dataset `retail_reviews.csv`, se explor√≥ la representaci√≥n vectorial del lenguaje y la clasificaci√≥n de sentimientos.

### 1.1. An√°lisis Sem√°ntico (Word2Vec)
Se entren√≥ un modelo Word2Vec sobre el corpus de rese√±as para capturar la sem√°ntica del negocio:
* **Similitud Sem√°ntica:** Para el t√©rmino **"r√°pido"**, el modelo identific√≥ t√©rminos como *recomendado, excelente, eficaz, entrega y puntual* con similitudes de hasta **0.99**.
* **Interpretaci√≥n Matem√°tica:** Se implement√≥ la **Similitud Coseno** en lugar del Producto Punto. Esto es fundamental en NLP porque la Similitud Coseno normaliza los vectores por su norma $L2$, permitiendo medir la cercan√≠a en √°ngulo y no por la magnitud (frecuencia) de las palabras.
* **√Ålgebra Vectorial:** Se valid√≥ la coherencia del espacio latente mediante analog√≠as, permitiendo entender relaciones entre atributos de productos y sentimientos.

### 1.2. Clasificaci√≥n de Sentimientos (Transformers vs Baseline)
Se compar√≥ un enfoque estad√≠stico tradicional contra un modelo de lenguaje avanzado:
* **Baseline (TF-IDF + LogReg):** Logr√≥ un F1-Score de **0.9000**.
* **BERT (paraphrase-multilingual-MiniLM-L12-v2):** Logr√≥ un F1-Score de **0.8924**.
* **Conclusi√≥n:** Aunque los puntajes son cercanos, el modelo basado en BERT demuestra una mejor capacidad de generalizaci√≥n ante estructuras ling√º√≠sticas complejas y sarcasmo.

---

## üìä Parte 2: Modelado de T√≥picos (BERTopic)
Se utiliz√≥ la arquitectura **BERTopic** para el descubrimiento autom√°tico de temas en las rese√±as.

* **Flujo T√©cnico:** Generaci√≥n de Embeddings ‚Üí Reducci√≥n de dimensionalidad (UMAP) ‚Üí Clustering (HDBSCAN) ‚Üí c-TF-IDF para la extracci√≥n de palabras clave.
* **T√≥picos Clave Identificados:**
    * **T√≥pico 0:** Satisfacci√≥n general y log√≠stica (palabras: lleg√≥, bien, r√°pido).
    * **T√≥pico 7:** Problemas cr√≠ticos con productos defectuosos o da√±ados.
    * **T√≥pico 11:** Malas experiencias con el servicio de soporte y atenci√≥n.



---

## üé¨ Parte 3: Recomendaci√≥n (Feedback Expl√≠cito)
Basado en `video_ratings.csv`, se desarroll√≥ un sistema para predecir la valoraci√≥n (1 a 5) de pel√≠culas.

* **Desaf√≠o de Sparsity:** Con ~20,000 registros para 600 usuarios y 150 pel√≠culas, el modelo utiliza t√©cnicas de **Filtrado Colaborativo** para predecir el inter√©s de un usuario en √≠tems que nunca ha consumido, optimizando la oferta de contenido.

---

## üéµ Parte 4: Recomendaci√≥n (Feedback Impl√≠cito)
An√°lisis de `music_logs.csv` utilizando el conteo de reproducciones como m√©trica de inter√©s.

* **M√©trica de Evaluaci√≥n:** Se implement√≥ la m√©trica **NDCG@K** (Normalized Discounted Cumulative Gain).
* **L√≥gica de Ranking:** El modelo eval√∫a si las canciones con m√°s "play counts" reales aparecen en los primeros lugares de la recomendaci√≥n.
* **F√≥rmula aplicada:** $$DCG_k = \sum_{i=1}^{k} \frac{rel_i}{\log_2(i + 1)}$$
  Donde $rel_i$ es la relevancia del √≠tem en la posici√≥n $i$. El resultado final se normaliza (NDCG) para comparar entre diferentes usuarios.

---

## üõ†Ô∏è Requisitos T√©cnicos y Compatibilidad

### ‚ö†Ô∏è Configuraci√≥n de Compatibilidad (Cr√≠tico)
Debido a que el entorno utiliza un Kernel de **Python 3.14.2** y **TensorFlow 2.16.2**, es estrictamente necesario forzar el uso de Keras Legacy para asegurar la estabilidad de los modelos:

```python
pip install tensorflow==2.16.2 tf_keras transformers sentence-transformers gensim scikit-learn bertopic plotly
import os
# Forzamos a TensorFlow a usar el motor antiguo de Keras 2
os.environ["TF_USE_LEGACY_KERAS"] = "1"

import tensorflow as tf
print(f"Versi√≥n de TF detectada: {tf.__version__}")

