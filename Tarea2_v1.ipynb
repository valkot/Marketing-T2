{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fcc7dc3",
   "metadata": {},
   "source": [
    "PARTE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb207c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 02:41:50.421368: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de TF: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Forzamos a TensorFlow a usar el motor antiguo de Keras 2\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras  # Usamos el paquete de compatibilidad\n",
    "import transformers\n",
    "\n",
    "print(f\"Versión de TF: {tf.__version__}\")\n",
    "# Debería mostrar algo como 2.16.x o 2.17.x pero usando el motor de tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1f253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.1.a) Términos Similares ---\n",
      "\n",
      "Similares a 'defectuoso':\n",
      " - insatisfecho: 0.9882\n",
      " - una: 0.9869\n",
      " - total: 0.9857\n",
      " - sucio: 0.9847\n",
      " - empaque: 0.9816\n",
      "\n",
      "Similares a 'rápido':\n",
      " - recomendado: 0.9850\n",
      " - llegó: 0.9830\n",
      " - estado: 0.9817\n",
      " - eficaz: 0.9810\n",
      " - y: 0.9784\n",
      "\n",
      "--- 1.1.c) Álgebra Vectorial (Analogía Retail) ---\n",
      "Error en analogía: \"Key 'negativo' not present in vocabulary\"\n",
      "\n",
      "Generando embeddings de BERT (esto puede tardar un poco)...\n",
      "\n",
      "==============================\n",
      "REPORTE DE COMPARACIÓN\n",
      "==============================\n",
      "F1-Score TF-IDF (Baseline): 0.9000\n",
      "F1-Score BERT Embeddings:    0.8924\n",
      "------------------------------\n",
      "\n",
      "--- Análisis de Fallos Específicos ---\n",
      "Texto: '  Producto duradero pero el precio es elevado.  '\n",
      "Real: 0.0 | TF-IDF predijo: 1.0 | BERT predijo: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ==========================================\n",
    "# 0. Carga y Preprocesamiento de Datos\n",
    "# ==========================================\n",
    "df = pd.read_csv('retail_reviews.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    # Eliminar caracteres especiales tipo #@#&* y números\n",
    "    text = re.sub(r'[^a-zA-ZáéíóúñÁÉÍÓÚÑ ]', '', text)\n",
    "    # Convertir a minúsculas y quitar espacios extra\n",
    "    return text.lower().strip()\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "# Eliminar filas vacías tras la limpieza\n",
    "df = df[df['text_clean'] != \"\"]\n",
    "\n",
    "# ==========================================\n",
    "# 1.1 Análisis de Embeddings (Word2Vec)\n",
    "# ==========================================\n",
    "\n",
    "# Tokenización para Word2Vec\n",
    "tokenized_corpus = [doc.split() for doc in df['text_clean']]\n",
    "\n",
    "# Entrenamiento del modelo Word2Vec\n",
    "# vector_size: dimensión del vector, window: contexto, min_count: frecuencia mínima\n",
    "w2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "print(\"--- 1.1.a) Términos Similares ---\")\n",
    "palabras_clave = [\"defectuoso\", \"rápido\"]\n",
    "for palabra in palabras_clave:\n",
    "    if palabra in w2v_model.wv:\n",
    "        similares = w2v_model.wv.most_similar(palabra, topn=5)\n",
    "        print(f\"\\nSimilares a '{palabra}':\")\n",
    "        for p, sim in similares:\n",
    "            print(f\" - {p}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\n--- 1.1.c) Álgebra Vectorial (Analogía Retail) ---\")\n",
    "# Analogía: \"excelente\" - \"positivo\" + \"negativo\" debería tender a algo como \"malo\" o \"pésimo\"\n",
    "try:\n",
    "    resultado_algebra = w2v_model.wv.most_similar(positive=['excelente', 'negativo'], negative=['positivo'], topn=1)\n",
    "    print(f\"Operación: 'excelente' - 'positivo' + 'negativo'\")\n",
    "    print(f\"Resultado semántico: {resultado_algebra[0][0]} (Similitud: {resultado_algebra[0][1]:.4f})\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error en analogía: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1.2 Clasificación de Texto\n",
    "# ==========================================\n",
    "\n",
    "# Preparación de etiquetas (Label Encoding manual para Positivo/Negativo)\n",
    "df['label'] = df['sentiment'].map({'Positivo': 1, 'Negativo': 0})\n",
    "df = df.dropna(subset=['label']) # Limpiar si hay etiquetas mal formadas\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text_clean'], df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Enfoque A: Baseline TF-IDF + Regresión Logística ---\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = model_lr.predict(X_test_tfidf)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "# --- Enfoque B: BERT Embeddings + Regresión Logística ---\n",
    "# Usamos un modelo ligero de BERT (paraphrase-multilingual-MiniLM-L12-v2)\n",
    "print(\"\\nGenerando embeddings de BERT (esto puede tardar un poco)...\")\n",
    "bert_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "X_train_bert = bert_model.encode(X_train.tolist())\n",
    "X_test_bert = bert_model.encode(X_test.tolist())\n",
    "\n",
    "model_bert = LogisticRegression()\n",
    "model_bert.fit(X_train_bert, y_train)\n",
    "y_pred_bert = model_bert.predict(X_test_bert)\n",
    "f1_bert = f1_score(y_test, y_pred_bert)\n",
    "\n",
    "# ==========================================\n",
    "# Reporte Final\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"REPORTE DE COMPARACIÓN\")\n",
    "print(\"=\"*30)\n",
    "print(f\"F1-Score TF-IDF (Baseline): {f1_tfidf:.4f}\")\n",
    "print(f\"F1-Score BERT Embeddings:    {f1_bert:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Identificar un caso donde TF-IDF falla y BERT acierta\n",
    "print(\"\\n--- Análisis de Fallos Específicos ---\")\n",
    "for i in range(len(y_test)):\n",
    "    idx = y_test.index[i]\n",
    "    if y_pred_tfidf[i] != y_test.iloc[i] and y_pred_bert[i] == y_test.iloc[i]:\n",
    "        print(f\"Texto: '{df.loc[idx, 'text']}'\")\n",
    "        print(f\"Real: {y_test.iloc[i]} | TF-IDF predijo: {y_pred_tfidf[i]} | BERT predijo: {y_pred_bert[i]}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
