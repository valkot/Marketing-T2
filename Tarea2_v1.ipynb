{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fcc7dc3",
   "metadata": {},
   "source": [
    "# PARTE I\n",
    "## Procesamiento de Lenguaje Natural (NLP) y *Embeddings*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcb207c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de TF: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Forzamos a TensorFlow a usar el motor antiguo de Keras 2, ya que no esta disponible para el kernel de python 3.14.2\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras  # Usamos el paquete de compatibilidad\n",
    "import transformers\n",
    "\n",
    "print(f\"Versión de TF: {tf.__version__}\")\n",
    "# Esto me debería mostrar algo como 2.16.x o 2.17.x pero usando el motor de tf-keras\n",
    "# Despues de correr esto podemos ejecutar el siguiente fragmento de codigo porque sin esto me seguira dando el error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f1f253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "====== 1. Carga y Preprocesamiento de Datos ======\n",
      "========================================\n",
      "========================================\n",
      "====== 1.1 Análisis de Embeddings (Word2Vec) ======\n",
      "========================================\n",
      "--- 1.1.a) Términos Similares ---\n",
      "\n",
      "Similares a 'defectuoso':\n",
      " - insatisfecho: 0.9883\n",
      " - una: 0.9868\n",
      " - total: 0.9857\n",
      " - sucio: 0.9846\n",
      " - decepción: 0.9820\n",
      "\n",
      "Similares a 'rápido':\n",
      " - recomendado: 0.9850\n",
      " - llegó: 0.9833\n",
      " - estado: 0.9818\n",
      " - eficaz: 0.9808\n",
      " - y: 0.9780\n",
      "\n",
      "--- 1.1.c) Álgebra Vectorial (Analogía Retail) ---\n",
      "Error en analogía: \"Key 'negativo' not present in vocabulary\"\n",
      "========================================\n",
      "====== 1.2 Clasificación de Texto ======\n",
      "========================================\n",
      "\n",
      "Generando embeddings de BERT (esto puede tardar un poco)...\n",
      "========================================\n",
      "====== Reporte Final del Item 1 ======\n",
      "========================================\n",
      "\n",
      "==============================\n",
      "REPORTE DE COMPARACIÓN\n",
      "==============================\n",
      "F1-Score TF-IDF (Baseline): 0.9000\n",
      "F1-Score BERT Embeddings:    0.8924\n",
      "------------------------------\n",
      "\n",
      "--- Análisis de Fallos Específicos ---\n",
      "Texto: '  Producto duradero pero el precio es elevado.  '\n",
      "Real: 0.0 | TF-IDF predijo: 1.0 | BERT predijo: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ==========================================\n",
    "# 1. Carga y Preprocesamiento de Datos\n",
    "# ==========================================\n",
    "print(\"=\" * 40)\n",
    "print(\"=\" * 6 + \" 1. Carga y Preprocesamiento de Datos \" + \"=\" * 6)\n",
    "print(\"=\" * 40)\n",
    "\n",
    "df = pd.read_csv('retail_reviews.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    # Eliminar caracteres especiales tipo #@#&* y números\n",
    "    text = re.sub(r'[^a-zA-ZáéíóúñÁÉÍÓÚÑ ]', '', text)\n",
    "    # Convertir a minúsculas y quitar espacios extra\n",
    "    return text.lower().strip()\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "# Eliminar filas vacías tras la limpieza\n",
    "df = df[df['text_clean'] != \"\"]\n",
    "\n",
    "# ==========================================\n",
    "# 1.1 Análisis de Embeddings (Word2Vec)\n",
    "# ==========================================\n",
    "print(\"=\" * 40)\n",
    "print(\"=\" * 6 + \" 1.1 Análisis de Embeddings (Word2Vec) \" + \"=\" * 6)\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Tokenización para Word2Vec\n",
    "tokenized_corpus = [doc.split() for doc in df['text_clean']]\n",
    "\n",
    "# Entrenamiento del modelo Word2Vec\n",
    "# vector_size: dimensión del vector, window: contexto, min_count: frecuencia mínima\n",
    "w2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "print(\"--- 1.1.a) Términos Similares ---\")\n",
    "palabras_clave = [\"defectuoso\", \"rápido\"]\n",
    "for palabra in palabras_clave:\n",
    "    if palabra in w2v_model.wv:\n",
    "        similares = w2v_model.wv.most_similar(palabra, topn=5)\n",
    "        print(f\"\\nSimilares a '{palabra}':\")\n",
    "        for p, sim in similares:\n",
    "            print(f\" - {p}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\n--- 1.1.c) Álgebra Vectorial (Analogía Retail) ---\")\n",
    "# Analogía: \"excelente\" - \"positivo\" + \"negativo\" debería tender a algo como \"malo\" o \"pésimo\"\n",
    "try:\n",
    "    resultado_algebra = w2v_model.wv.most_similar(positive=['excelente', 'negativo'], negative=['positivo'], topn=1)\n",
    "    print(f\"Operación: 'excelente' - 'positivo' + 'negativo'\")\n",
    "    print(f\"Resultado semántico: {resultado_algebra[0][0]} (Similitud: {resultado_algebra[0][1]:.4f})\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error en analogía: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1.2 Clasificación de Texto\n",
    "# ==========================================\n",
    "print(\"=\" * 40)\n",
    "print(\"=\" * 6 + \" 1.2 Clasificación de Texto \" + \"=\" * 6)\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Preparación de etiquetas (Label Encoding manual para Positivo/Negativo)\n",
    "df['label'] = df['sentiment'].map({'Positivo': 1, 'Negativo': 0})\n",
    "df = df.dropna(subset=['label']) # Limpiar si hay etiquetas mal formadas\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text_clean'], df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Enfoque A: Baseline TF-IDF + Regresión Logística ---\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = model_lr.predict(X_test_tfidf)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "# --- Enfoque B: BERT Embeddings + Regresión Logística ---\n",
    "# Usamos un modelo ligero de BERT (paraphrase-multilingual-MiniLM-L12-v2)\n",
    "print(\"\\nGenerando embeddings de BERT (esto puede tardar un poco)...\")\n",
    "bert_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "X_train_bert = bert_model.encode(X_train.tolist())\n",
    "X_test_bert = bert_model.encode(X_test.tolist())\n",
    "\n",
    "model_bert = LogisticRegression()\n",
    "model_bert.fit(X_train_bert, y_train)\n",
    "y_pred_bert = model_bert.predict(X_test_bert)\n",
    "f1_bert = f1_score(y_test, y_pred_bert)\n",
    "\n",
    "# ==========================================\n",
    "# Reporte Final del Item 1\n",
    "# ==========================================\n",
    "print(\"=\" * 40)\n",
    "print(\"=\" * 6 + \" Reporte Final del Item 1 \" + \"=\" * 6)\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"REPORTE DE COMPARACIÓN\")\n",
    "print(\"=\"*30)\n",
    "print(f\"F1-Score TF-IDF (Baseline): {f1_tfidf:.4f}\")\n",
    "print(f\"F1-Score BERT Embeddings:    {f1_bert:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Identificar un caso donde TF-IDF falla y BERT acierta\n",
    "print(\"\\n--- Análisis de Fallos Específicos ---\")\n",
    "for i in range(len(y_test)):\n",
    "    idx = y_test.index[i]\n",
    "    if y_pred_tfidf[i] != y_test.iloc[i] and y_pred_bert[i] == y_test.iloc[i]:\n",
    "        print(f\"Texto: '{df.loc[idx, 'text']}'\")\n",
    "        print(f\"Real: {y_test.iloc[i]} | TF-IDF predijo: {y_pred_tfidf[i]} | BERT predijo: {y_pred_bert[i]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948ca09",
   "metadata": {},
   "source": [
    "# PARTE II\n",
    "## Topic Modeling (No supervisado)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7db31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "====== 2.1 Carga y Limpieza (Igual que Parte 1) ======\n",
      "======================================================\n",
      "=====================================================\n",
      "====== 2.2 Configuración del Pipeline BERTopic ======\n",
      "=====================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607a6f8a78ad486cb07f76fb3b5d70f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Instalación de librerías (\n",
    "# Para poder ejecutar este fragmento de codigo se tiene que instalar la siguiente libreria \n",
    "# !pip install bertopic sentence-transformers pandas\n",
    "\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. Carga y Limpieza (Igual que Parte 1)\n",
    "# ==========================================\n",
    "print(\"=\" * 54)\n",
    "print(\"=\" * 6 + \" 2.1 Carga y Limpieza (Igual que Parte 1) \" + \"=\" * 6)\n",
    "print(\"=\" * 54)\n",
    "\n",
    "df = pd.read_csv('retail_reviews.csv')\n",
    "\n",
    "# Limpieza básica para quitar ruido visual\n",
    "def clean_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = re.sub(r'[^a-zA-ZáéíóúñÁÉÍÓÚÑ ]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "docs = df[df['text_clean'] != \"\"]['text_clean'].tolist()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Configuración del Pipeline BERTopic\n",
    "# ==========================================\n",
    "print(\"=\" * 53)\n",
    "print(\"=\" * 6 + \" 2.2 Configuración del Pipeline BERTopic \" + \"=\" * 6)\n",
    "print(\"=\" * 53)\n",
    "\n",
    "# Paso 1: Embeddings (Multilingüe para Español)\n",
    "# Usamos un modelo que soporte español para captar semántica correcta\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Paso 2: UMAP (Reducción de Dimensionalidad)\n",
    "# n_neighbors=15: Balance entre estructura local y global\n",
    "# n_components=5: Reducimos a 5 dimensiones para que HDBSCAN trabaje bien\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "\n",
    "# Paso 3: HDBSCAN (Clusterización)\n",
    "# min_cluster_size=10: Queremos tópicos con al menos 10 reseñas\n",
    "# prediction_data=True: Para poder predecir nuevos documentos luego\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Entrenamiento del Modelo\n",
    "# ==========================================\n",
    "print(\"=\" * 42)\n",
    "print(\"=\" * 6 + \" 2.3 Entrenamiento del Modelo \" + \"=\" * 6)\n",
    "print(\"=\" * 42)\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model, # Paso 1\n",
    "    umap_model=umap_model,           # Paso 2\n",
    "    hdbscan_model=hdbscan_model,     # Paso 3\n",
    "    language=\"multilingual\",         # Refuerzo para stopwords en español\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# ==========================================\n",
    "# 4. Resultados e Interpretación de Negocio\n",
    "# ==========================================\n",
    "print(\"=\" * 56)\n",
    "print(\"=\" * 6 + \" 2.4 Resultados e Interpretación de Negocio \" + \"=\" * 6)\n",
    "print(\"=\" * 56)\n",
    "\n",
    "# Generaramos la tabla de Top 5 Tópicos\n",
    "freq = topic_model.get_topic_info()\n",
    "print(\"\\n--- Top 5 Tópicos Encontrados ---\")\n",
    "print(freq.head(6)) # head(6) porque el primero suele ser el -1 (Ruido)\n",
    "\n",
    "# Mostrar palabras clave del Tópico 0 (el más frecuente)\n",
    "print(\"\\n--- Palabras Clave del Tópico Principal (ID 0) ---\")\n",
    "print(topic_model.get_topic(0))\n",
    "\n",
    "# Visualización pora que nos funcione en Jupyter/Colab\n",
    "topic_model.visualize_barchart(top_n_topics=5)\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26d122",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE III\n",
    "## Sistemas de Recomendación: Filtrado Colaborativo Explícito\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8f61d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "==== 3.1 Factorización Matricial (SVD) ====\n",
      "===========================================\n",
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.6901  0.7290  0.6838  0.6826  0.7052  0.6982  0.0174  \n",
      "MAE (testset)     0.3815  0.3809  0.3605  0.3661  0.3805  0.3739  0.0088  \n",
      "Fit time          0.20    0.18    0.17    0.17    0.35    0.21    0.07    \n",
      "Test time         0.02    0.02    0.02    0.02    0.02    0.02    0.00    \n",
      "\n",
      "> RMSE Promedio tras 5-fold: 0.6982\n",
      "============================================\n",
      "====== 3.2 El problema del Cold-Start ======\n",
      "============================================\n",
      "RMSE: 0.1747\n",
      "RMSE para usuarios Cold-Start: 0.1747\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3.1 Factorización Matricial (SVD)\n",
    "# ==========================================\n",
    "print(\"=\" * 43)\n",
    "print(\"=\" * 4 + \" 3.1 Factorización Matricial (SVD) \" + \"=\" * 4)\n",
    "print(\"=\" * 43)\n",
    "\n",
    "# 1. Carga y Limpieza (Eliminar outliers como el 999 detectado mas arriba)\n",
    "df_ratings = pd.read_csv('video_ratings.csv')\n",
    "df_ratings = df_ratings[df_ratings['rating'] <= 5] # Filtramos solo valores 1-5\n",
    "\n",
    "# 2. Configurar Surprise\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_ratings[['user_id', 'movie_id', 'rating']], reader)\n",
    "\n",
    "# 3. Implementación de SVD y Validación Cruzada (5-fold)\n",
    "algo = SVD()\n",
    "results = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "rmse_promedio = results['test_rmse'].mean()\n",
    "print(f\"\\n> RMSE Promedio tras 5-fold: {rmse_promedio:.4f}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3.2 El problema del Cold-Start\n",
    "# ==========================================\n",
    "print(\"=\" * 44)\n",
    "print(\"=\" * 6 + \" 3.2 El problema del Cold-Start \" + \"=\" * 6)\n",
    "print(\"=\" * 44)\n",
    "\n",
    "# Dividir en entrenamiento y prueba (80/20)\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Identificar usuarios fríos (< 3 interacciones en el set original)\n",
    "user_counts = df_ratings['user_id'].value_counts()\n",
    "cold_users = user_counts[user_counts < 3].index\n",
    "\n",
    "# Filtrar predicciones de test que corresponden a usuarios fríos\n",
    "cold_predictions = [p for p in predictions if p.uid in cold_users]\n",
    "\n",
    "# Calcular RMSE para este subgrupo\n",
    "rmse_cold = accuracy.rmse(cold_predictions)\n",
    "print(f\"RMSE para usuarios Cold-Start: {rmse_cold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb0202",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE IV\n",
    "## Sistemas de Recomendación: Feedback Implícito y LTR\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42e6d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "====== 4.1 Mínimos Cuadrados Alternados (ALS) ======\n",
      "====================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1344d93686c4ce6bf461d22226258a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "====== 4.2 Evaluación de Ranking (NDCG) ======\n",
      "==============================================\n",
      "Resultados de Evaluación de Ranking:\n",
      "Relevancia real en el Top-5: [1, 0, 1, 1, 0]\n",
      "NDCG@5 obtenido: 0.9060\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 4.1 Mínimos Cuadrados Alternados (ALS)\n",
    "# ==========================================\n",
    "print(\"=\" * 52)\n",
    "print(\"=\" * 6 + \" 4.1 Mínimos Cuadrados Alternados (ALS) \" + \"=\" * 6)\n",
    "print(\"=\" * 52)\n",
    "\n",
    "# Paso opcional: Evita problemas de hilos en Anaconda/Mac\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "# 1. Limpieza y conversión de tipos\n",
    "df_music = pd.read_csv('music_logs.csv').dropna()\n",
    "df_music = df_music[df_music['play_count'] > 0].copy()\n",
    "\n",
    "# 2. Crear la matriz Ítem-Usuario directamente\n",
    "# Convertimos play_count a float32 para silenciar el Warning que nos aparecia\n",
    "user_items = sparse.csr_matrix((\n",
    "    df_music['play_count'].astype('float32'), \n",
    "    (df_music['song_id'].astype(int), df_music['user_id'].astype(int))\n",
    "))\n",
    "\n",
    "# 3. Entrenar el modelo\n",
    "model = AlternatingLeastSquares(factors=64, regularization=0.1, iterations=20)\n",
    "model.fit(user_items) \n",
    "\n",
    "# ==========================================\n",
    "# 4.2 Evaluación de Ranking (NDCG)\n",
    "# ==========================================\n",
    "print(\"=\" * 46)\n",
    "print(\"=\" * 6 + \" 4.2 Evaluación de Ranking (NDCG) \" + \"=\" * 6)\n",
    "print(\"=\" * 46)\n",
    "\n",
    "def calculate_ndcg_at_k(relevance_scores, k):\n",
    "    \"\"\"\n",
    "    Calcula el NDCG a un nivel K.\n",
    "    relevance_scores: Lista de 1s (relevante) y 0s (no relevante) en el orden recomendado.\n",
    "    \"\"\"\n",
    "    # 1. Calcular DCG@K\n",
    "    # Formula: sum(rel_i / log2(i + 1))\n",
    "    dcg = sum([rel / np.log2(idx + 2) for idx, rel in enumerate(relevance_scores[:k])])\n",
    "    \n",
    "    # 2. Calcular IDCG@K (El caso ideal: todos los aciertos al principio)\n",
    "    ideal_relevance = sorted(relevance_scores, reverse=True)\n",
    "    idcg = sum([rel / np.log2(idx + 2) for idx, rel in enumerate(ideal_relevance[:k])])\n",
    "    \n",
    "    # 3. Calcular NDCG\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "\n",
    "# Suponiendo que el modelo nos recomendó 5 canciones y así se ven en el 'test set':\n",
    "# Canción 1: Escuchada (1), Canción 2: No (0), Canción 3: Escuchada (1), ...\n",
    "real_user_relevance = [1, 0, 1, 1, 0] \n",
    "\n",
    "ndcg_5 = calculate_ndcg_at_k(real_user_relevance, k=5)\n",
    "\n",
    "print(f\"Resultados de Evaluación de Ranking:\")\n",
    "print(f\"Relevancia real en el Top-5: {real_user_relevance}\")\n",
    "print(f\"NDCG@5 obtenido: {ndcg_5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120b0af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE V\n",
    "## Desafíos avanzados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6e33e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================\n",
      "========== 5.1. NLP: Análisis de Sentimiento Basado en Aspectos (ABSA) ==========\n",
      "=================================================================================\n",
      "Reseñas mixtas detectadas: 226\n",
      "                                                 text sentiment\n",
      "0   Atención al cliente pésima. Producto de mala c...  Negativo\n",
      "2     Atención al cliente pésima. Producto de mala...  Negativo\n",
      "6   Producto caro y muy lento en funcionar. Gran d...  Negativo\n",
      "11  Producto caro y muy lento en funcionar. Gran d...  Negativo\n",
      "23  Atención al cliente pésima. Producto de mala c...  Negativo\n",
      "===================================================================\n",
      "========== 5.2. RecSys: Sesgo de Popularidad y Long Tail ==========\n",
      "===================================================================\n",
      "Cobertura de Catálogo: 77.69%\n",
      "===========================================================================\n",
      "========== 5.3. RecSys: Re-ranking Híbrido por Margen Financiero ==========\n",
      "===========================================================================\n",
      "   movie_id margin  score_orig  score_final\n",
      "5       125   High    3.318060     3.981672\n",
      "0        28    Low    3.385831     3.047248\n",
      "1       234    Low    3.359567     3.023611\n",
      "2        31    Low    3.336855     3.003169\n",
      "3       245    Low    3.333733     3.000360\n",
      "4         1    Low    3.324766     2.992290\n",
      "6        96    Low    3.315660     2.984094\n",
      "7       156    Low    3.298640     2.968776\n",
      "8        74    Low    3.290440     2.961396\n",
      "9       134    Low    3.282997     2.954698\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import SVD, Dataset, Reader\n",
    "\n",
    "# ==============================================================================\n",
    "# 5.1. NLP: Análisis de Sentimiento Basado en Aspectos (ABSA)\n",
    "# ==============================================================================\n",
    "print(\"=\" * 81)\n",
    "print(\"=\" * 10 + \" 5.1. NLP: Análisis de Sentimiento Basado en Aspectos (ABSA) \" + \"=\" * 10)\n",
    "print(\"=\" * 81)\n",
    "\n",
    "# Vocabularios de polaridad (Lógica de palabras exactas)\n",
    "palabras_positivas = ['excelente', 'bueno', 'buena', 'gran', 'calidad', 'recomiendo', 'perfecto', 'rapido', 'satisfecho']\n",
    "palabras_negativas = ['malo', 'mala', 'defectuoso', 'tardo', 'lento', 'roto', 'pobre', 'problema', 'decepcion', 'pesimo']\n",
    "\n",
    "# Función para detectar señales mixtas usando split() para evitar falsos positivos\n",
    "def es_mixto(texto):\n",
    "    if pd.isna(texto): return False\n",
    "    tokens = set(texto.lower().split())\n",
    "    tiene_pos = any(pos in tokens for pos in palabras_positivas)\n",
    "    tiene_neg = any(neg in tokens for neg in palabras_negativas)\n",
    "    return tiene_pos and tiene_neg\n",
    "\n",
    "# Filtrar y mostrar resultados\n",
    "df['es_mixto'] = df['text_clean'].apply(es_mixto)\n",
    "df_mixto = df[df['es_mixto']].copy()\n",
    "\n",
    "print(f\"Reseñas mixtas detectadas: {len(df_mixto)}\")\n",
    "print(df_mixto[['text', 'sentiment']].head(5))\n",
    "\n",
    "# ==============================================================================\n",
    "# 5.2. RecSys: Sesgo de Popularidad y Long Tail\n",
    "# ==============================================================================\n",
    "print(\"=\" * 67)\n",
    "print(\"=\" * 10 + \" 5.2. RecSys: Sesgo de Popularidad y Long Tail \" + \"=\" * 10)\n",
    "print(\"=\" * 67)\n",
    "\n",
    "# Entrenamos SVD rápido con los datos de la Parte 3\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data_svd = Dataset.load_from_df(df_ratings[['user_id', 'movie_id', 'rating']], reader)\n",
    "trainset = data_svd.build_full_trainset()\n",
    "algo_svd = SVD(random_state=42)\n",
    "algo_svd.fit(trainset)\n",
    "\n",
    "# Calculamos cobertura para una muestra de usuarios\n",
    "usuarios_test = df_ratings['user_id'].unique()[:100]\n",
    "items_totales = set(df_ratings['movie_id'].unique())\n",
    "items_recomendados = set()\n",
    "\n",
    "for u_id in usuarios_test:\n",
    "    # Predecir para todos los items y tomar Top 10\n",
    "    preds = [(i_id, algo_svd.predict(u_id, i_id).est) for i_id in items_totales]\n",
    "    preds.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_10 = [x[0] for x in preds[:10]]\n",
    "    items_recomendados.update(top_10)\n",
    "\n",
    "cobertura = (len(items_recomendados) / len(items_totales)) * 100\n",
    "print(f\"Cobertura de Catálogo: {cobertura:.2f}%\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5.3. RecSys: Re-ranking Híbrido por Margen Financiero\n",
    "# ==============================================================================\n",
    "print(\"=\" * 75)\n",
    "print(\"=\" * 10 + \" 5.3. RecSys: Re-ranking Híbrido por Margen Financiero \" + \"=\" * 10)\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# 1. Limpieza de Metadata\n",
    "df_meta = pd.read_csv('movie_metadata.csv')\n",
    "df_meta['margin_category'] = df_meta['margin_category'].fillna('Low').str.strip().str.title()\n",
    "# Corregir el caso específico 'High' vs 'High' que detectamos\n",
    "df_meta['margin_category'] = df_meta['margin_category'].replace({'High': 'High', 'High': 'High', 'High': 'High'})\n",
    "\n",
    "# 2. Función de Re-ranking\n",
    "def aplicar_reranking(user_id, n=10):\n",
    "    # Obtener predicciones originales\n",
    "    preds = [(i_id, algo_svd.predict(user_id, i_id).est) for i_id in items_totales]\n",
    "    preds.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_original = preds[:n]\n",
    "    \n",
    "    # Aplicar pesos de margen\n",
    "    lista_final = []\n",
    "    for m_id, score in top_original:\n",
    "        categoria = df_meta[df_meta['movie_id'] == m_id]['margin_category'].values[0] if m_id in df_meta['movie_id'].values else 'Low'\n",
    "        \n",
    "        factor = 1.2 if categoria == 'High' else 0.9\n",
    "        score_final = score * factor\n",
    "        lista_final.append({'movie_id': m_id, 'margin': categoria, 'score_orig': score, 'score_final': score_final})\n",
    "    \n",
    "    return pd.DataFrame(lista_final).sort_values('score_final', ascending=False)\n",
    "\n",
    "# Ejecutar para el usuario 260\n",
    "df_resultado = aplicar_reranking(user_id=260)\n",
    "print(df_resultado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
